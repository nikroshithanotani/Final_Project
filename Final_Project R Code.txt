############################################################################################################ Case - 1 ###########################################################################################################################Libraries Requiredlibrary(ggplot2)library(gridExtra)library(dplyr)library(caret)library(rpart)library(rpart.plot)library(gains)NPL.df <- read.csv("North-Point List.csv")#############################  Missing Values ##################################missing_values <-is.na(NPL.df)col_missing_count <- colSums(missing_values)print(col_missing_count)#############################  Zero Values #####################################non_zero_purchase_zero_spending <- with(NPL.df , Purchase !=0 & Spending == 0)count_non_zero_purchase_zero_spending <- sum(non_zero_purchase_zero_spending)print(count_non_zero_purchase_zero_spending)'Data Analysis'################### Categorical Variables Analysis ############################categorical_columns <- c('US', 'source_a', 'source_c', 'source_b', 'source_d', 'source_e',                         'source_m', 'source_o', 'source_h', 'source_r', 'source_s', 'source_t',                         'source_u', 'source_p', 'source_x', 'source_w', 'Web.order', 'Gender.male',                         'Address_is_res', 'Purchase')theme_set(theme_minimal())plot_histogram <- function(col) {  ggplot(NPL.df, aes(x = as.factor(.data[[col]]))) +    geom_bar(fill = "yellow", color = "black", alpha = 0.7) +    labs(title = paste("Histogram -", col)) +    theme(axis.text.x = element_text(angle = 45, hjust = 1))}plots <- lapply(categorical_columns, function(col) {  if (length(unique(NPL.df[[col]])) > 1) {    plot_histogram(col)  } else {    ggplot() + ggtitle(paste("Histogram -", col)) +      theme_minimal()  }})grid.arrange(grobs = plots, ncol = 4)'Plot 2'source_columns <- NPL.df[, c("source_a", "source_c", "source_b", "source_d", "source_e", "source_m", "source_o", "source_h", "source_r", "source_s", "source_t", "source_u", "source_p", "source_x", "source_w")]source_counts <- colSums(source_columns == 1)print(source_counts)source_columns <- NPL.df[, c("source_a", "source_c", "source_b", "source_d", "source_e", "source_m", "source_o", "source_h", "source_r", "source_s", "source_t", "source_u", "source_p", "source_x", "source_w")]source_counts <- colSums(source_columns == 1)source_counts_df <- data.frame(Source = names(source_counts), Count = source_counts)histogram <- ggplot(source_counts_df, aes(x = Source, y = Count)) +  geom_bar(stat = "identity", fill = "skyblue") +  labs(title = "Counts of Occurrences of '1' for Each Source Column",       x = "Source", y = "Count") +  theme_minimal() +  theme(axis.text.x = element_text(angle = 45, hjust = 1))print(histogram)'Plot 3'filtered_data <- subset(NPL.df, Purchase == 1)percentage_data <- data.frame(table(filtered_data$Web.order) / nrow(filtered_data) * 100)ggplot(percentage_data, aes(x = Var1, y = Freq, fill = as.factor(Var1))) +  geom_bar(stat = "identity", position = "dodge", fill = c("pink", "purple")) +  # Specify fill colors here  geom_text(aes(label = sprintf("%.1f%%", Freq)),            position = position_dodge(width = 0.9), vjust = -0.5) +  labs(title = "Bar Plot of Web Order when Purchase is 1",       x = "Web Order", y = "Percentage", fill = "Web Order") +  theme_minimal()######################## Numerical Variables Analysis ##########################'1'NPL.df$days_difference <- NPL.df$X1st_update_days_ago - NPL.df$last_update_days_agoplot_spending <- ggplot(NPL.df, aes(x = Spending, fill = ..count..)) +  geom_histogram(bins = 30, alpha = 0.7) +  ylab("Count") +  scale_fill_gradient("Count", low = "red", high = "purple") +  theme_minimal()plot_freq <- ggplot(NPL.df, aes(x = Freq, fill = ..count..)) +  geom_histogram(bins = 30, alpha = 0.7) +  ylab("Count") +  scale_fill_gradient("Count", low = "red", high = "purple") +  theme_minimal()plot_difference <- ggplot(NPL.df, aes(x = days_difference, fill = ..count..)) +  geom_histogram(bins = 30, alpha = 0.7) +  ylab("Count") +  scale_fill_gradient("Count", low = "red", high = "purple") +  theme_minimal()grid.arrange(plot_spending, plot_freq, plot_difference, ncol = 3)'2'purchased_data <- NPL.df %>% filter(Purchase == 1)purchased_data$days_difference <- purchased_data$X1st_update_days_ago - purchased_data$last_update_days_agoplot_spending <- ggplot(purchased_data, aes(x = Spending, fill = ..count..)) +  geom_histogram(bins = 30, alpha = 0.7) +  ylab("Count") +  scale_fill_gradient("Count", low = "pink", high = "purple") +  theme_minimal()plot_freq <- ggplot(purchased_data, aes(x = Freq, fill = ..count..)) +  geom_histogram(bins = 30, alpha = 0.7) +  ylab("Count") +  scale_fill_gradient("Count", low = "pink", high = "purple") +  theme_minimal()plot_difference <- ggplot(purchased_data, aes(x = days_difference, fill = ..count..)) +  geom_histogram(bins = 30, alpha = 0.7) +  ylab("Count") +  scale_fill_gradient("Count", low = "pink", high = "purple") +  theme_minimal()grid.arrange(plot_spending, plot_freq, plot_difference, ncol = 3)'3'ggplot(NPL.df, aes(x = Spending, y = Purchase)) +  geom_point(aes(color = factor(Purchase))) +  labs(title = "Scatter Plot of Amount Spent and Purchase",       x = "Amount Spent",       y = "Purchase",       color = "Purchase Status") +  scale_color_manual(values = c("red", "green"), labels = c("Non-Purchase", "Purchase")) +  theme_minimal()##########################Data Transformation##################################NPL.df <- NPL.df[, -1]NPL.df$days_difference <- NPL.df$X1st_update_days_ago - NPL.df$last_update_days_agocolnames(NPL.df) <- gsub("Gender.male", "Gender is male", colnames(NPL.df))colnames(NPL.df) <- gsub("Web.order", "Web_order", colnames(NPL.df))colnames(NPL.df) <- gsub("last_update_days_ago", "Last_Interaction", colnames(NPL.df))colnames(NPL.df) <- gsub("X1st_update_days_ago", "First_Interaction", colnames(NPL.df))colnames(NPL.df) <- gsub("Spending", "Amount_Spent", colnames(NPL.df))print(NPL.df)write.csv(NPL.df, "North_Point.csv", row.names = FALSE)########################## Model Building ####################################### Goal 1 - ClassificationNP.df <- read.csv("North_Point.csv")'Logistic Regression'set.seed(2024)indices <- sample(1:2000)train_data <- NP.df[indices[1:800], ]validation_data <- NP.df[indices[801:1500], ]holdout_data <- NP.df[indices[1501:2000], ]dim(train_data)dim(holdout_data)dim(validation_data)# Fit the logistic regression model on the training dataformula <- as.formula("Purchase ~ source_a + source_c + source_b + source_d + source_e + source_m + source_o + source_h + source_r + source_s + source_t + source_u + source_p + source_x + source_w + Gender.is.male + Web_order + Freq + Address_is_res + days_difference")logistic_model <- glm(formula, data = train_data, family = "binomial")validation_data$predicted_purchase <- predict(logistic_model, newdata = validation_data, type = "response")validation_data$predicted_purchase <- ifelse(as.numeric(validation_data$predicted_purchase) >= 0.5, "1", "0")validation_data$Purchase <- factor(validation_data$Purchase, levels = c("1","0"))validation_data$predicted_purchase <- factor(validation_data$predicted_purchase, levels = c("1","0"))#Confusion matrixconf_matrix <- confusionMatrix(validation_data$Purchase, validation_data$predicted_purchase, positive = "1")conf_matrix# Perform stepwise backward variable selection using AICbackward_model <- step(logistic_model, direction = "backward", trace = 0)summary(backward_model)# Fit logistic regression model with stepwise variable selectionformula_Bic <- as.formula("Purchase ~ source_a + source_h + source_r + source_u + source_w + Web_order + Freq + Address_is_res")logistic_model_Stepwise <- glm(formula_Bic, data = train_data, family = "binomial")validation_data$predicted_purchase_stepwise <- predict(logistic_model_Stepwise, newdata = validation_data, type = "response")validation_data$predicted_purchase_stepwise <- as.factor(ifelse(validation_data$predicted_purchase_stepwise >= 0.5, 1, 0))validation_data$Purchase <- as.factor(validation_data$Purchase)validation_data$Purchase <- factor(validation_data$Purchase, levels = c("1","0"))validation_data$predicted_purchase_stepwise <- factor(validation_data$predicted_purchase_stepwise, levels = c("1","0"))# Calculate confusion matrixconf_matrix1 <- confusionMatrix(validation_data$Purchase, validation_data$predicted_purchase_stepwise, positive = "1")conf_matrix1'Classification Tree'# Fit the Classification Tree on the training dataformula_tree <- as.formula("Purchase ~ source_a + source_h + source_r + source_u + source_w + Web_order + Freq + Address_is_res")tree_model <- rpart(formula_tree, data = train_data, method = "class", control = rpart.control(minsplit = 5, cp = 0.01))# Plot the treerpart.plot(tree_model, main = "Classification Tree")validation_data$predicted_purchase <- predict(tree_model, newdata = validation_data, type = "class")validation_data$Purchase <- factor(validation_data$Purchase, levels = c(1, 0))validation_data$predicted_purchase <- factor(validation_data$predicted_purchase, levels = c(1, 0))# Confusion matrixconf_matrix_tree <- confusionMatrix(validation_data$Purchase, validation_data$predicted_purchase, positive = "1")conf_matrix_tree'KNN'# Fit the KNN on the training dataformula_knn <- as.formula("Purchase ~ source_a + source_h + source_r + source_u + source_w + Web_order + Freq + Address_is_res")X_train <- model.matrix(formula_knn, data = train_data)[,-1]  X_valid <- model.matrix(formula_knn, data = validation_data)[,-1]k_neighbors <- 7library(class)# Building the KNN modelknn_model <- knn(train = X_train, test = X_valid, cl = train_data$Purchase, k = k_neighbors)validation_data$Purchase<- factor(validation_data$Purchase, levels = c("1","0"))validation_data$predicted_purchase  <- factor(validation_data$predicted_purchase , levels = c("1","0"))# Confusion matrixconf_matrix_knn <- confusionMatrix(validation_data$Purchase, validation_data$predicted_purchase, positive = "1")conf_matrix_knn#Evaluation on Holdout with best model - Stepwise Logistic Modelformula_Bic <- as.formula("Purchase ~ source_a + source_h + source_r + source_u + source_w + Web_order + Freq + Address_is_res")logistic_model_Stepwise <- glm(formula_Bic, data = holdout_data, family = "binomial")holdout_data$predicted_purchase_binary <- predict(logistic_model_Stepwise, newdata = holdout_data, type = "response")holdout_data$predicted_purchase_binary <- as.factor(ifelse(holdout_data$predicted_purchase_binary >= 0.5, 1, 0))holdout_data$Purchase <- as.factor(holdout_data$Purchase)holdout_data$predicted_purchase_binary <- as.factor(holdout_data$predicted_purchase_binary)holdout_data$Purchase<- factor(holdout_data$Purchase, levels = c("1","0"))holdout_data$predicted_purchase_binary  <- factor(holdout_data$predicted_purchase_binary , levels = c("1","0"))# Confusion matrix conf_matrix_test <- confusionMatrix(holdout_data$Purchase, holdout_data$predicted_purchase_binary, positive = "1")conf_matrix_test# Goal 2 Regression'Linear_Regression'# Data partitioningNP.df <- read.csv("North_Point.csv")set.seed(2024)selected_features <- c('source_a', 'source_c', 'source_b', 'source_d', 'source_e', 'source_m', 'source_o', 'source_h', 'source_r', 'source_s', 'source_t', 'source_u', 'source_p', 'source_x', 'source_w', 'Gender.is.male', 'Web_order', 'Freq', 'First_Interaction', 'Address_is_res')indices <- sample(which(NP.df$Purchase == 1))train_prop <- 0.4validation_prop <- 0.35holdout_prop <- 0.25train_size <- floor(train_prop * length(indices))validation_size <- floor(validation_prop * length(indices))holdout_size <- length(indices) - train_size - validation_sizetrain_indices <- indices[1:train_size]validation_indices <- indices[(train_size + 1):(train_size + validation_size)]holdout_indices <- indices[(train_size + validation_size + 1):length(indices)]train_data_lm <- NP.df[train_indices, ]validation_data_lm <- NP.df[validation_indices, ]holdout_data_lm <- NP.df[holdout_indices, ]dim(train_data_lm)dim(validation_data_lm)dim(holdout_data_lm)# Define the independent variables (X) and the dependent variable (y) for training setX_train <- train_data_lm[, selected_features]y_train <- train_data_lm$Amount_Spent# Define the independent variables (X) and the dependent variable (y) for validation setX_validation <- validation_data_lm[, selected_features]y_validation <- validation_data_lm$Amount_Spent# Create and fit the linear regression model using the training setmodel <- lm(y_train ~ ., data = cbind(y_train, X_train))y_pred_validation <- predict(model, newdata = cbind(X_validation))mae_validation <- mean(abs(y_pred_validation - y_validation))print(paste("Validation MAE:", mae_validation))validation_rmse <- sqrt(mean((y_pred_validation - y_validation)^2))print(paste("Validation RMSE:", validation_rmse))dependent_variable <- "Amount_Spent"selected_features1 <- c('source_h', 'Freq', 'First_Interaction', 'Address_is_res')initial_formula <- as.formula(paste(dependent_variable, "~", paste(selected_features, collapse = " + ")))# Perform stepwise forward selectionstepwise_model <- step(lm(initial_formula, data = NP.df), direction = "backward", trace = 0)summary(stepwise_model)X_train1 <- train_data_lm[, selected_features1]y_train1 <- train_data_lm$Amount_Spentmodel <- lm(y_train1 ~ ., data = cbind(y_train1, X_train1))X_validation1 <- validation_data_lm[, selected_features1]y_validation1 <- validation_data_lm$Amount_Spenty_pred_validation1 <- predict(model, newdata = cbind(X_validation1))mae_validation1 <- mean(abs(y_pred_validation1 - y_validation1))print(paste("Validation MAE:", mae_validation1))'Regression tree'# Fit the regression treetree_model <- rpart(Amount_Spent ~ ., data = NP.df, method = "anova")rpart.plot(tree_model)predicted_amount_spent <- predict(tree_model, newdata = holdout_data_lm)mae_tree <- mean(abs(predicted_amount_spent - holdout_data_lm$Amount_Spent))print(paste("Regression Tree MAE:", mae_tree))## Holdout testing for Selected Model for Goal 2 - stepwise forward selectionX_holdout <- holdout_data[, selected_features]y_pred_holdout <- predict(model, newdata = cbind(X_holdout))mae_holdout <- mean(abs(y_pred_holdout - holdout_data$Amount_Spent))print(paste("Holdout MAE:", mae_holdout))'Gross Profit'purchasers <- NP.df$Amount_Spent[NP.df$Purchase == 1]purchasers_numeric <- purchasers[!is.na(purchasers) & is.numeric(purchasers)]mean_spending_purchase <- mean(purchasers_numeric)remaining_customers <- 180000markup_percentage <- 0.053mailing_cost_per_booklet <- 2gross_profit <- remaining_customers * (markup_percentage * mean_spending_purchase - mailing_cost_per_booklet)gross_profit'Adding Columns according to Business requirement 'dim(holdout_data)holdout_data <- holdout_data[, !(names(holdout_data) == "predicted_purchase_binary")]# Predict purchase probability using the Purchase.glm modelholdout_data$predicted <- predict(logistic_model_Stepwise, holdout_data, type = 'response')head(holdout_data)# Adjust predicted purchase probability by multiplying by 0.1065holdout_data$AdjustedProbPurchase <- holdout_data$predicted * 0.1065head(holdout_data)# Predicted_Spendingholdout_purchasers <- holdout_dataholdout_purchasers$pred_spend <- predict(model, newdata = holdout_data)head(holdout_purchasers)# Calculate expected spending by multiplying predicted spending with adjusted purchase probabilityholdout_purchasers$expectedSpending <- holdout_purchasers$pred_spend * holdout_purchasers$AdjustedProbPurchase# Display column names of holdout_purchasersnames(holdout_purchasers)# Calculate the sum of expected spendingsum(holdout_purchasers$expectedSpending)'Gain Chart'library(gains)gain <- gains(holdout_purchasers$Amount_Spent, holdout_purchasers$expectedSpending)df <- data.frame(  ncases=c(0, gain$cume.obs),  cumSpending=c(0, gain$cume.pct.of.total * sum(holdout_purchasers$expectedSpending)))ggplot(df, aes(x=ncases, y=cumSpending)) +  geom_line() +  geom_line(data=data.frame(ncases=c(0, nrow(holdout_purchasers)), cumSpending=c(0, sum(holdout_purchasers$expectedSpending))),            color="blue", linetype=2) + # adds baseline  labs(x="Observation ", y="Cumulative Expected Spending", title="Cumulative gains chart") +  scale_y_continuous(labels = scales::comma)############################################################################################################ Case - 2 ################################################################################################################################################## Reading Data File #####################################MP.df <- read.csv("used_device_data.csv")library(ggplot2)library(dplyr)library(gplots)library(rpart)library(gridExtra)########################## Missing Values ######################################missing_values <- colSums(is.na(MP.df))print("Missing values per column:")print(missing_values)numeric_features <- c("screen_size", "rear_camera_mp", "front_camera_mp", "internal_memory", "ram", "battery", "weight", "days_used", "normalized_used_price", "normalized_new_price")data_numeric <- MP.df [, numeric_features]# Perform k-means clusteringk <- 3  set.seed(2024)  kmeans_result <- kmeans(na.omit(data_numeric), centers = k)cluster_assignment <- kmeans_result$clustercluster_centroids <- aggregate(. ~ cluster_assignment, data  = cbind(cluster_assignment, na.omit(data_numeric)), mean)# Replace missing values with cluster centroidsfor (feature in numeric_features) {  missing_indices <- is.na(MP.df [[feature]])  cluster_values <- cluster_centroids[cluster_assignment[missing_indices], feature]  MP.df [missing_indices, feature] <- cluster_values}print(colSums(is.na(MP.df )))rear_camera_median <- median(MP.df $rear_camera_mp, na.rm = TRUE)MP.df $rear_camera_mp[is.na(MP.df $rear_camera_mp)] <- rear_camera_medianprint(colSums(is.na(MP.df )))####################Checking for Zero##############################################'Check for zero values in the data frame'zero_values <- sapply(MP.df, function(x) sum(x == 0, na.rm = TRUE))print("Zero values per column:")print(zero_values)devices_with_zero_front_cam <- MP.df[MP.df$front_camera_mp == 0, ]# Print the brand names of devices with front camera resolution of 0cat("Brand names of devices with front camera resolution of 0:\n")cat(devices_with_zero_front_cam$device_brand)########################### Numerical Attributes Analysis#######################'Plot 1'# Select numerical variablesnumerical_vars <- dplyr::select(MP.df, screen_size, rear_camera_mp, front_camera_mp, internal_memory, ram, battery, weight, release_year, days_used, normalized_used_price, normalized_new_price)detect_outliers <- function(x) {  q1 <- quantile(x, 0.25)  q3 <- quantile(x, 0.75)  iqr <- q3 - q1  lower_fence <- q1 - 1.5 * iqr  upper_fence <- q3 + 1.5 * iqr  outliers <- x[x < lower_fence | x > upper_fence]  return(outliers)}outliers <- lapply(numerical_vars, detect_outliers)outliers_df <- data.frame(variable = rep(names(outliers), sapply(outliers, length)),                          value = unlist(outliers))boxplot <- ggplot() +  geom_boxplot(data = MP.df, aes(y = normalized_used_price)) +  geom_point(data = outliers_df, aes(x = variable, y = value), color = "red", size = 2) +  facet_wrap(~variable, scales = "free") +  labs(title = "Boxplot of Numerical Variables with Outliers Highlighted", y = "Value") +  theme_minimal()print(boxplot)'Plot 2'# Create the plotggplot(MP.df, aes(x = battery, y = weight)) +  geom_point(color = "red", alpha = 0.6) +  labs(x = "Battery Capacity", y = "Weight", title = "Battery Capacity vs. Device Weight") +  theme_minimal()'Plot 3'# Create the plotggplot(MP.df, aes(x = days_used)) +  geom_histogram(binwidth = 50, fill = "skyblue", color = "black") +  labs(x = "Days Used", y = "Frequency", title = "Distribution of Days Used for Devices") +  theme_minimal()'Plot 4'# Create scatter plot for screen size vs. weightplot_screen_size <- ggplot(MP.df, aes(x = screen_size, y = weight)) +  geom_point(color = "blue", alpha = 0.6) +  labs(x = "Screen Size", y = "Weight") +  theme_minimal()# Create scatter plot for internal memory vs. weightplot_internal_memory <- ggplot(MP.df, aes(x = internal_memory, y = weight)) +  geom_point(color = "green", alpha = 0.6) +  labs(x = "Internal Memory", y = "Weight") +  theme_minimal()# Create scatter plot for RAM vs. weightplot_ram <- ggplot(MP.df, aes(x = ram, y = weight)) +  geom_point(color = "orange", alpha = 0.6) +  labs(x = "RAM", y = "Weight") +  theme_minimal()grid.arrange(plot_screen_size, plot_internal_memory, plot_ram, ncol = 3)'Plot 5'# Density plot for normalized used price and normalized new priceggplot(MP.df, aes(x = normalized_new_price, fill = "Normalized New Price")) +  geom_density(alpha = 0.5) +  geom_density(aes(x = normalized_used_price, fill = "Normalized Used Price"), alpha = 0.5) +  labs(title = "Density Plot of Normalized Prices",       x = "Normalized Price",       y = "Density") +  scale_fill_manual(values = c("Normalized New Price" = "blue", "Normalized Used Price" = "red")) +  theme_minimal()# Calculate the correlation coefficientcorrelation <- cor(MP.df$normalized_new_price, MP.df$normalized_used_price)print(correlation)############################Categorical Analysis################################'Plot 1'# Create the bar plot with vertical x-axis labelsggplot(MP.df, aes(x = device_brand)) +  geom_bar(fill = "purple", color = "black") +  labs(title = "Frequency Distribution of Device Brands", x = "Device Brand", y = "Frequency") +  theme_minimal() +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))'Plot 2'# Create a bar plot for OS types with countggplot(MP.df, aes(x = os)) +  geom_bar(fill = "yellow", color = "black") +  geom_text(stat='count', aes(label=..count..), vjust=-0.5, size=3, color="black") +  labs(title = "Distribution of Operating System Types",       x = "Operating System",       y = "Count") +  theme_minimal()num_4g <- sum(MP.df$'X4g' == 'yes')num_5g <- sum(MP.df$'X5g' == 'yes')connectivity_counts <- data.frame(  Connectivity = c("X4G", "X5G"),  Count = c(num_4g, num_5g))'Plot 3'# Create a bar plot with counts displayed on ggplot(connectivity_counts, aes(x = Connectivity, y = Count, fill = Connectivity)) +  geom_bar(stat = "identity") +  geom_text(aes(label = Count), vjust = -0.5, size = 3, color = "black") +  labs(title = "Distribution of Connectivity Types",       x = "Connectivity",       y = "Count") +  theme_minimal()'Plot 4'# Create a bar plot with counts displayed on top of the barsggplot(connectivity_counts, aes(x = Connectivity, y = Count, fill = Connectivity)) +  geom_bar(stat = "identity") +  geom_text(aes(label = Count), vjust = -0.5, size = 3, color = "black") +  labs(title = "Distribution of Connectivity Types",       x = "Connectivity Type",       y = "Count") +  scale_fill_manual(values = c("X4G" = "skyblue", "X5G" = "orange")) +  theme_minimal()########################LASSO MODEL FOR PREDICTOR ANALYSIS #####################library(glmnet)X <- model.matrix(normalized_used_price ~ . - device_brand, data = MP.df)y <- MP.df$normalized_used_priceset.seed(2024)lasso_model <- cv.glmnet(X, y, alpha = 1) print(lasso_model)# Perform cross-validation for ridge regressioncv.rrfit <- cv.glmnet(X, y, alpha = 1)  # Plot cross-validation curveplot(cv.rrfit)# Fit LASSO regression modellasso_model <- glmnet(X, y, alpha = 1)# Plot LASSO coefficient profilesplot(lasso_model, xvar = "lambda", label = TRUE)lambda_min <- cv.rrfit$lambda.minlambda_1se <- cv.rrfit$lambda.1seabline(v = log(lambda_min), col = "red", lty = 2)abline(v = log(lambda_1se), col = "blue", lty = 2)# Adding labelstext(log(lambda_min), max(lasso_model$beta), labels = expression(lambda[min]), pos = 4, col = "red")text(log(lambda_1se), max(lasso_model$beta), labels = expression(lambda[X1se]), pos = 4, col= "blue")predictor_names <- colnames(X)predictor_names###############################DATA TRANSFORMATON################################ Dummies for OSos_dummies <- model.matrix(~ os - 1, data = MP.df)MP.df <- cbind(MP.df, os_dummies)MP.df <- MP.df[, -which(names(MP.df) == "os")]# Renaming 4g and 5gMP.df$connectivity <- ifelse(MP.df$`X4g` == "yes" | MP.df$`X5g` == "yes", "Yes", "No")# Dividing the devices by its screen sizeMP.df$device_type <- ifelse(MP.df$screen_size <= 7.6, "Mobile",                            ifelse(MP.df$screen_size > 7.6 & MP.df$screen_size <= 14, "Tablet",                                   ifelse(MP.df$screen_size > 14 & MP.df$screen_size <= max(MP.df$screen_size), "2-in-1 Convertible", NA)))MP.df <- subset(MP.df, select = -c(X4g, X5g))print(MP.df)##############################. DATA PARTITION. ################################total_obs <- 3454# Calculate the number of observations for each partitiontrain_size <- round(0.5 * total_obs)validation_size <- round(0.3 * total_obs)test_size <- round(0.2 * total_obs)set.seed(2024)# Generate indices for random samplingindices <- sample(1:total_obs)train_data <- MP.df[indices[1:train_size], ]validation_data <- MP.df[indices[(train_size + 1):(train_size + validation_size)], ]test_data <- MP.df[indices[(train_size + validation_size + 1):(train_size + validation_size + test_size)], ]cat("Training set size:", dim(train_data)[1], "\n")cat("Validation set size:", dim(validation_data)[1], "\n")cat("Test set size:", dim(test_data)[1], "\n")######################################  REGRESSION  ############################'STEPWISE MODEL '# Remove device_brand column using subset()UP.df <- subset(MP.df, select = -device_brand)lm_model <- lm(normalized_used_price ~ ., data = UP.df)# Summary of the linear regression modelsummary(lm_model)selected_data <- UP.df[c("screen_size", "rear_camera_mp", "front_camera_mp", "internal_memory", "ram", "weight", "release_year", "osOthers", "normalized_used_price")]# Build linear regression model# Perform stepwise regression on the selected predictorsstepwise_selected_model <- step(lm(normalized_used_price ~ ., data = selected_data), direction = "both", trace = FALSE)# Summary of the stepwise regression model with selected predictorssummary(stepwise_selected_model)# Make predictions on the validation set using the stepwise selected modelvalidation_predictions <- predict(stepwise_selected_model, newdata = validation_data)# Calculate MAE on the validation setvalidation_mae <- caret::MAE(validation_predictions, validation_data$normalized_used_price)cat("Validation MAE after stepwise regression with selected predictors:", validation_mae, "\n")'REGRESSION TREE'set.seed(2024)regression_tree_model_selected <- rpart(normalized_used_price ~ ., data = selected_data)summary(regression_tree_model_selected)validation_predictions_selected <- predict(regression_tree_model_selected, newdata = validation_data)validation_mae_selected <- caret::MAE(validation_predictions_selected, validation_data$normalized_used_price)cat("Validation MAE using regression tree with selected variables:", validation_mae_selected, "\n")'TEST PREDICTION ON SELECTED MODEL _ STEPWISE'# Make predictions on the test set using the stepwise selected modeltest_predictions <- predict(stepwise_selected_model, newdata = test_data)# Calculate MAE on the test settest_mae <- caret::MAE(test_predictions, test_data$normalized_used_price)cat("Test MAE after stepwise regression with selected predictors:", test_mae, "\n")#############################   CLASSIFICATION.   ################################ Make predictions on the entire dataset using the stepwise selected modelMP.df$predicted_Used_Price <- predict(stepwise_selected_model, newdata = MP.df)head(MP.df)# Calculate the difference between the normalized new and used pricesMP.df$price_difference <- MP.df$normalized_new_price - MP.df$predicted_Used_Price# Define thresholds for depreciation categoriesdepreciation_threshold_low <- quantile(MP.df$price_difference, probs = 1/3)depreciation_threshold_high <- quantile(MP.df$price_difference, probs = 2/3)# Classify depreciation into low, medium, and high categoriesMP.df$depreciation_category <- ifelse(MP.df$price_difference <= depreciation_threshold_low, "low",                                      ifelse(MP.df$price_difference <= depreciation_threshold_high, "medium", "high"))# Classify release year into categoriesMP.df$release_year_category <- ifelse(MP.df$release_year <= 2016, "old", "new")# Classify days used into categoriesMP.df$days_used_category <- ifelse(MP.df$days_used <= 273, "short-term", "long-term")# Create a new column 'predicted_price' based on the conditionsMP.df <- MP.df %>%  mutate(used_price_classification = case_when(    MP.df$depreciation_category == "low" & MP.df$release_year_category == "old" & MP.df$days_used_category == "short-term" ~ "low",    MP.df$depreciation_category == "medium" & MP.df$release_year_category == "new" & MP.df$days_used_category == "short-term" ~ "medium",    MP.df$depreciation_category == "high" & MP.df$release_year_category == "new" & MP.df$days_used_category == "long-term" ~ "high",    MP.df$depreciation_category == "medium" & MP.df$release_year_category == "old" & MP.df$days_used_category == "long-term" ~ "medium",    MP.df$depreciation_category == "low" & MP.df$release_year_category == "new" & MP.df$days_used_category == "long-term" ~ "low",    MP.df$depreciation_category == "low" & MP.df$release_year_category == "old" & MP.df$days_used_category == "long-term" ~ "low",    MP.df$depreciation_category == "low" & MP.df$release_year_category == "new" & MP.df$days_used_category == "short-term" ~ "low",    # Add more conditions as needed    TRUE ~ "high"  # Default value if none of the conditions are met  ))print(MP.df)write.csv(MP.df, "Predictions", row.names = FALSE)# Group by predicted category and device brand, count occurrences, and arrange in descending orderbrand_counts <- MP.df %>%  group_by(used_price_classification, device_brand) %>%  summarise(count = n()) %>%  arrange(used_price_classification, desc(count))# Extract top 5 brands for each predicted categorytop_low_brands <- brand_counts %>% filter(used_price_classification == "low") %>% slice_head(n = 5)top_medium_brands <- brand_counts %>% filter(used_price_classification == "medium") %>% slice_head(n = 5)top_high_brands <- brand_counts %>% filter(used_price_classification == "high") %>% slice_head(n = 5)# Create plots for each predicted categoryplot_low <- ggplot(top_low_brands, aes(x = reorder(device_brand, count), y = count, fill = device_brand)) +  geom_bar(stat = "identity") +  labs(title = "Top 5 Brands in Low Predicted Category",       x = "Device Brand",       y = "Count of Devices") +  theme(axis.text.x = element_text(angle = 45, hjust = 1))plot_medium <- ggplot(top_medium_brands, aes(x = reorder(device_brand, count), y = count, fill = device_brand)) +  geom_bar(stat = "identity") +  labs(title = "Top 5 Brands in Medium Predicted Category",       x = "Device Brand",       y = "Count of Devices") +  theme(axis.text.x = element_text(angle = 45, hjust = 1))plot_high <- ggplot(top_high_brands, aes(x = reorder(device_brand, count), y = count, fill = device_brand)) +  geom_bar(stat = "identity") +  labs(title = "Top 5 Brands in High Predicted Category",       x = "Device Brand",       y = "Count of Devices") +  theme(axis.text.x = element_text(angle = 45, hjust = 1))# Reset the graphics devicedev.off()# View plots againplot_lowplot_mediumplot_high############################################################################################################ Case - 3 #########################################################################################################################################################Libraries#####################################library(ggplot2)library(dplyr)library(gridExtra)library(glmnet)library(rpart)library(caret)library(corrplot)library(randomForest)Mortgage.df <- read.csv("Mortgage.csv")head(Mortgage.df)##############################Missing Values#################################missing_values <- sapply(Mortgage.df, function(x) sum(is.na(x)))print(missing_values)missing_values <- is.na(Mortgage.df$LTV_time)missing_values_LTV <- is.na(Mortgage.df$LTV_time)Mortgage.df$LTV_time[missing_values_LTV] <- 0missing_values <- sapply(Mortgage.df, function(x) sum(is.na(x)))print(missing_values)##############################Zero Values######################################zero_values <- sapply(Mortgage.df[,-c(12,13,14,15,21,22,23)], function(x) sum(x == 0, na.rm = TRUE))print("Zero values per column:")print(zero_values)###########################Data Preparation for Preprocessing##################'Add a new column Observation_Count indicating the count of observations for each borrower (ID)'Mortgage.df <- Mortgage.df %>%  group_by(id) %>%  mutate(Observation_Count = n()) %>%  ungroup()'Group by ID and status_time, count the observations for each combination'observation_counts <- Mortgage.df %>%  group_by(id, status_time) %>%  summarise(Observation_Count = n()) %>%  ungroup()'Identify borrowers with only one observation for default or paid loan'single_record_borrowers <- observation_counts %>%  group_by(id) %>%  filter(n() == 1 & (status_time == 1 | status_time == 2)) %>%  pull(id)Mortgage.df_filtered <- Mortgage.df %>%  filter(!(id %in% single_record_borrowers))print(head(Mortgage.df_filtered))'Convert orig_time to actual timestamp based on time column'Mortgage.df$orig_time <- Mortgage.df$time + Mortgage.df$orig_time*3  Mortgage.df$loan_duration_days <- as.numeric(Mortgage.df$mat_time - Mortgage.df$orig_time)Mortgage.df$loan_duration_years <- Mortgage.df$loan_duration_days / 365.25  # Accounting for leap yearsprint(head(Mortgage.df))'Calculate the difference in balance between consecutive observations'Mortgage.df$difference_in_balance <- c(NA, Mortgage.df$balance_time[-1] - Mortgage.df$balance_time[-nrow(Mortgage.df)])Mortgage.df$missing_payments <- ifelse(Mortgage.df$difference_in_balance > 0, 0, abs(Mortgage.df$difference_in_balance))Mortgage.df$missing_payments[is.na(Mortgage.df$missing_payments)] <- 0print(head(Mortgage.df))all_same <- function(x) length(unique(x)) == 1'Group the data by ID and check if interest rate is fixed or variable for each ID'Mortgage.df <- Mortgage.df %>%  group_by(id) %>%  mutate(interest_rate_category = ifelse(all_same(interest_rate_time), 1, 0)) %>%  ungroup()print(head(Mortgage.df))'Calculate the difference between each row in balance_time for each ID'Mortgage.df <- Mortgage.df %>%  arrange(id, time) %>%  group_by(id) %>%  mutate(Missing_Payments = sum(balance_time - lag(balance_time) == 0, na.rm = TRUE))head(as.data.frame(Mortgage.df))Mortgage.df <- Mortgage.df %>%  mutate(REtype_combined = ifelse(REtype_CO_orig_time == 1, "CO",                                  ifelse(REtype_PU_orig_time == 1, "PU", "SF")))'Calculate the average LTV for each ID'Mortgage.df <- Mortgage.df %>%  group_by(id) %>%  mutate(avg_LTV = mean(LTV_time, na.rm = TRUE))Mortgage.df <- Mortgage.df %>%  group_by(id) %>%  summarise(    across(.cols = c(orig_time, first_time, balance_time, interest_rate_time, hpi_time, gdp_time, uer_time, investor_orig_time,                      balance_orig_time, FICO_orig_time, LTV_orig_time, hpi_orig_time, status_time,                      Missing_Payments, avg_LTV, interest_rate_category,REtype_combined), .fns = last),    .groups = "drop"  )'# Select the last observation for each id'Mortgage_Filtered.df <- Mortgage.df %>%   group_by(id) %>%   slice(n()) %>%    ungroup()##########################################Data Analysis#######################'Separating Numerical and Categorical Attributes and analysis them'Numerical_attributes = Mortgage_Filtered.df[,-c(18)]Categorical_attributes <- Mortgage_Filtered.df[, c(18)]'1'hist(Mortgage_Filtered.df$interest_rate_category, main = "Fixed / Variable", xlab = "interest rate", col = "purple")'2'ggplot(Mortgage_Filtered.df, aes(x = balance_orig_time)) +  geom_histogram(binwidth = 500, fill = "skyblue", color = "red") +  labs(x = "Orig Time", y = "Frequency", title = "Histogram of Orig Time")'3'ggplot(Mortgage_Filtered.df, aes(x = REtype_combined, fill = REtype_combined)) +  geom_bar() +  labs(x = "Property Type", y = "Frequency", title = "Combined Property Type Graph") +  scale_fill_manual(values = c("CO" = "skyblue", "PU" = "lightgreen", "SF" = "pink"))'4'hist(Mortgage_Filtered.df$status_time, main = "Histogram to know number of borrowers per status", xlab = "Status", col = "pink")'5'hist(Mortgage_Filtered.df$uer_time, main = "Histogram of uer", xlab = "uer_time", col = "purple")'6'hist(Mortgage_Filtered.df$investor_orig_time, main = "Histogram of mat_time", xlab = "mat_time", col = "green")# Compute correlation matrixcorrelation_matrix <- cor(Mortgage_Filtered.df[, c( "orig_time", "first_time", "balance_time",  "interest_rate_time", "hpi_time", "gdp_time", "uer_time",  "investor_orig_time", "balance_orig_time", "FICO_orig_time", "LTV_orig_time", "hpi_orig_time")])# Print correlation matrixprint(correlation_matrix)corrplot(correlation_matrix, method = "color")##########################Data Partitioning##############################'Goal 1 'paidoff_default <- Mortgage_Filtered.df %>%  filter(status_time %in% c(1, 2))Holdout.df <- Mortgage_Filtered.df %>%  filter(!status_time %in% c(1, 2))head(paidoff_default)head(Holdout.df)table(paidoff_default$status_time)table(Holdout.df$status_time)paidoff_default$status_time <- ifelse(paidoff_default$status_time == 2, 0, paidoff_default$status_time)table(paidoff_default$status_time)set.seed(2024)train_rows <- sample(rownames(paidoff_default), nrow(paidoff_default) * 0.5)validation_rows <- sample(setdiff(rownames(paidoff_default), train_rows),                          nrow(paidoff_default) * 0.3)head(Holdout.df)train_data1 <- paidoff_default[train_rows,]Validation_data1 <- paidoff_default[validation_rows,]# Verify the sizes of each setnrow(train_data1)  table(train_data1$status_time)nrow(Validation_data1)nrow(Holdout.df)##################################Model_Building###############################'#########################For Goal 1 #################################### ''logistic regression model'logistic_model_1 <- glm(status_time ~ ., data = train_data1, family = "binomial")logistic_predictions_1 <- predict(logistic_model_1, newdata = Validation_data1, type = "response")logistic_predictions_1 <- ifelse(logistic_predictions_1 > 0.5, 1, 0) actual_labels <- Validation_data1$status_time'confusion matrix'conf_matrix <- confusionMatrix(factor(actual_labels), factor(logistic_predictions_1), positive = "1")print(conf_matrix)'Classification Tree'tree_model_1 <- rpart(status_time ~ ., data = train_data1, method = "class")tree_predictions_1 <- predict(tree_model_1, newdata = Validation_data1, type = "class")tree_prediction_1 <- predict(tree_model_1, newdata = Validation_data1, type = "class")tree_confusion_matrix_1 <- confusionMatrix(factor(Validation_data1$status_time), factor(tree_predictions_1))print("Confusion Matrix for Classification Tree:")print(tree_confusion_matrix_1)'#############################For Goal 2##################################''Data Partition'set.seed(2024)  # For reproducibilitytrain_index <- sample(1:nrow(Mortgage_Filtered.df), 0.7 * nrow(Mortgage_Filtered.df))  # 70% train, 30% holdouttrain_data <- Mortgage_Filtered.df[train_index, ]validation_data <- Mortgage_Filtered.df[-train_index, ]train_data$status_time <- ifelse(train_data$status_time == 2, 1, 0)summary(train_data$status_time)nrow(train_data)  nrow(validation_data)'logistic regression model'logistic_model_2 <- glm(status_time ~ ., data = train_data, family = "binomial")logistic_predictions_2 <- predict(logistic_model_2, newdata = train_data, type = "response")logistic_predictions_2 <- ifelse(logistic_predictions_2 > 0.5, 1, 0) actual_labels <- train_data$status_timeconf_matrix <- confusionMatrix(factor(actual_labels), factor(logistic_predictions_2), positive = "1")print(conf_matrix)'Classification Tree'tree_model_2 <- rpart(status_time ~ ., data = train_data, method = "class")tree_predictions_2<- predict(tree_model_2, newdata = train_data, type = "class")tree_predictions_2 <- predict(tree_model_2, newdata = train_data, type = "class")tree_confusion_matrix_2 <- confusionMatrix(factor(train_data$status_time), factor(tree_predictions_2))print("Confusion Matrix for Classification Tree:")print(tree_confusion_matrix_2)###########################################Holdout Predictions################### Holdout for Goal 1 holdout_predictions_1 <- predict(logistic_model_1, newdata = Holdout.df, type = "response")holdout_predictions_1 <- ifelse(holdout_predictions_1 > 0.5, 1, 0)Holdout_predict_1 = factor(holdout_predictions_1 , levels = c("0","1"), labels = c("Paidoff","Default"))print("Predictions for Logistic Regression Model on Holdout Data:")table(Holdout_predict_1)# Holdout for Goal 2 Classification_predict_2 = predict(tree_model_2, newdata = Holdout.df, type = "class")Classification_predict_2 = factor(Classification_predict_2, levels = c("0","1"), labels = c("Initiate Loan ","No Loan"))print(table(Classification_predict_2))################################# END ############################################